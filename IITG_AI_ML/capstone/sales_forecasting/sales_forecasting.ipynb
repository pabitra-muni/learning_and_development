{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4417e59b",
   "metadata": {},
   "source": [
    "# AIML Capstone Project: Sales Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d658d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babd558e",
   "metadata": {},
   "source": [
    "# 1. Preliminary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513f7a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. Import the datasets\n",
    "restaurants = pd.read_csv('resturants.csv')  # Note: filename has typo in original\n",
    "items = pd.read_csv('items.csv')\n",
    "sales = pd.read_csv('sales.csv')\n",
    "\n",
    "print(\"Restaurants shape:\", restaurants.shape)\n",
    "print(\"Items shape:\", items.shape)\n",
    "print(\"Sales shape:\", sales.shape)\n",
    "\n",
    "# b. Examine structure and outliers\n",
    "print(\"\\n--- Basic Info ---\")\n",
    "print(sales.info())\n",
    "print(items.info())\n",
    "print(restaurants.info())\n",
    "\n",
    "print(\"\\n--- Sales Sample ---\")\n",
    "print(sales.head())\n",
    "\n",
    "# Fix date format (dd/mm/yy -> datetime)\n",
    "sales['date'] = pd.to_datetime(sales['date'], format='%d/%m/%y')\n",
    "\n",
    "# Check for outliers in item_count and price\n",
    "print(\"\\n--- Outlier Check ---\")\n",
    "print(\"Item count > 1000?\", (sales['item_count'] > 1000).sum())\n",
    "print(\"Price > 100?\", (sales['price'] > 100).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4f3c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# c. Merge all datasets\n",
    "# sales has item_id (not item name), so rename column\n",
    "sales = sales.rename(columns={'item': 'item_id'})\n",
    "\n",
    "# Merge sales with items to get item details and store_id\n",
    "df = sales.merge(items, left_on='item_id', right_on='id', suffixes=('', '_item'))\n",
    "df = df.rename(columns={'store_id': 'store_id'})\n",
    "\n",
    "# Merge with restaurants to get store name\n",
    "df = df.merge(restaurants, left_on='store_id', right_on='id', suffixes=('', '_store'))\n",
    "df = df.rename(columns={'name': 'store_name'})\n",
    "\n",
    "# Drop redundant id columns\n",
    "df = df.drop(['id_item', 'id_store'], axis=1)\n",
    "\n",
    "# Create sales amount\n",
    "df['sales_amount'] = df['price'] * df['item_count']\n",
    "\n",
    "print(\"\\nMerged dataset shape:\", df.shape)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfd457e",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07661759",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert date to datetime and extract features\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day'] = df['date'].dt.day\n",
    "df['day_of_week'] = df['date'].dt.day_name()\n",
    "df['quarter'] = df['date'].dt.quarter\n",
    "df['month_name'] = df['date'].dt.month_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e841ae95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. Overall date-wise sales pattern\n",
    "daily_sales = df.groupby('date')['item_count'].sum().reset_index()\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(daily_sales['date'], daily_sales['item_count'], linewidth=1)\n",
    "plt.title('Daily Total Item Sales Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Total Items Sold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c36b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b. Sales fluctuation across days of the week\n",
    "dow_sales = df.groupby('day_of_week')['item_count'].sum().reindex(\n",
    "    ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "dow_sales.plot(kind='bar')\n",
    "plt.title('Total Sales by Day of Week')\n",
    "plt.ylabel('Total Items Sold')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfca7c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c. Monthly trends\n",
    "monthly_sales = df.groupby(['year', 'month_name'])['item_count'].sum().reset_index()\n",
    "monthly_order = ['January', 'February', 'March', 'April', 'May', 'June',\n",
    "                 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "for year in monthly_sales['year'].unique():\n",
    "    data = monthly_sales[monthly_sales['year'] == year]\n",
    "    data = data.set_index('month_name').reindex(monthly_order).reset_index()\n",
    "    plt.plot(data['month_name'], data['item_count'], marker='o', label=year)\n",
    "plt.legend()\n",
    "plt.title('Monthly Sales Trend by Year')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Total Items Sold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12093cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d. Quarterly sales averaged over years\n",
    "quarterly = df.groupby('quarter')['item_count'].mean()\n",
    "plt.figure(figsize=(8,5))\n",
    "quarterly.plot(kind='bar')\n",
    "plt.title('Average Quarterly Sales')\n",
    "plt.xlabel('Quarter')\n",
    "plt.ylabel('Average Items Sold per Day')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fef7471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# e. Restaurant performance comparison\n",
    "store_performance = df.groupby('store_name').agg({\n",
    "    'item_count': 'sum',\n",
    "    'sales_amount': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "print(\"\\n=== Restaurant with Highest Sales Volume ===\")\n",
    "print(store_performance.sort_values('item_count', ascending=False))\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "store_performance.sort_values('item_count', ascending=False).plot(\n",
    "    x='store_name', y='item_count', kind='bar', legend=False)\n",
    "plt.title('Total Items Sold by Restaurant')\n",
    "plt.ylabel('Total Items')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac76ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales by store over years\n",
    "store_year = df.groupby(['store_name', 'year'])['item_count'].sum().unstack()\n",
    "store_year.plot(kind='bar', figsize=(12,6))\n",
    "plt.title('Yearly Sales by Restaurant')\n",
    "plt.ylabel('Total Items Sold')\n",
    "plt.legend(title='Year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09baf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f. Most popular items overall and per store\n",
    "overall_popular = df.groupby(['name'])['item_count'].sum().sort_values(ascending=False).head(10)\n",
    "print(\"\\nTop 10 Most Popular Items Overall:\")\n",
    "print(overall_popular)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "overall_popular.plot(kind='bar')\n",
    "plt.title('Top 10 Most Popular Items Overall')\n",
    "plt.ylabel('Total Quantity Sold')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cff378b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most popular item at each store\n",
    "popular_per_store = df.groupby(['store_name', 'name'])['item_count'].sum().reset_index()\n",
    "popular_per_store = popular_per_store.loc[popular_per_store.groupby('store_name')['item_count'].idxmax()]\n",
    "print(\"\\nMost Popular Item at Each Store:\")\n",
    "print(popular_per_store[['store_name', 'name', 'item_count']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4446649c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# g. Is the store with highest volume also making most money per day?\n",
    "daily_revenue = df.groupby(['date', 'store_name'])['sales_amount'].sum().reset_index()\n",
    "avg_daily_revenue = daily_revenue.groupby('store_name')['sales_amount'].mean()\n",
    "\n",
    "print(\"\\nAverage Daily Revenue per Store:\")\n",
    "print(avg_daily_revenue.sort_values(ascending=False))\n",
    "\n",
    "print(\"\\nStore with highest volume also highest daily revenue?\",\n",
    "      store_performance.loc[store_performance['item_count'].idxmax(), 'store_name'],\n",
    "      \"->\", avg_daily_revenue.idxmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d1c2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# h. Most expensive item at each restaurant and its calories\n",
    "expensive_per_store = df.loc[df.groupby('store_name')['cost'].idxmax()][['store_name', 'name', 'cost', 'kcal']]\n",
    "print(\"\\nMost Expensive Item per Restaurant and its Calories:\")\n",
    "print(expensive_per_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b654d9e",
   "metadata": {},
   "source": [
    "# 3. Forecasting using Machine Learning (Linear Reg, RF, XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021d7651",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Prepare aggregated daily total sales for forecasting\n",
    "daily_total = df.groupby('date').agg({\n",
    "    'item_count': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "daily_total['year'] = daily_total['date'].dt.year\n",
    "daily_total['month'] = daily_total['date'].dt.month\n",
    "daily_total['day'] = daily_total['date'].dt.day\n",
    "daily_total['day_of_week'] = daily_total['date'].dt.dayofweek\n",
    "daily_total['quarter'] = daily_total['date'].dt.quarter\n",
    "daily_total['is_weekend'] = daily_total['day_of_week'].isin([5,6]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bfd643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lag features\n",
    "for lag in [1,3,7,14,30]:\n",
    "    daily_total[f'lag_{lag}'] = daily_total['item_count'].shift(lag)\n",
    "\n",
    "daily_total = daily_total.dropna()\n",
    "\n",
    "# Features and target\n",
    "features = ['year','month','day','day_of_week','quarter','is_weekend',\n",
    "            'lag_1','lag_3','lag_7','lag_14','lag_30']\n",
    "X = daily_total[features]\n",
    "y = daily_total['item_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e441850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split: last 6 months as test\n",
    "max_date = daily_total['date'].max()\n",
    "six_months_ago = max_date - pd.DateOffset(months=6)\n",
    "\n",
    "train = daily_total[daily_total['date'] <= six_months_ago]\n",
    "test = daily_total[daily_total['date'] > six_months_ago]\n",
    "\n",
    "X_train, X_test = train[features], test[features]\n",
    "y_train, y_test = train['item_count'], test['item_count']\n",
    "\n",
    "print(f\"Training period: {train['date'].min()} to {train['date'].max()}\")\n",
    "print(f\"Testing period : {test['date'].min()} to {test['date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7d69a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'XGBoost': XGBRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
    "    results[name] = rmse\n",
    "    print(f\"{name} RMSE: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac8c80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model\n",
    "best_model_name = min(results, key=results.get)\n",
    "best_model = models[best_model_name]\n",
    "print(f\"\\nBest performing model: {best_model_name} with RMSE {results[best_model_name]:.2f}\")\n",
    "\n",
    "# Forecast next 365 days using best model\n",
    "future_dates = pd.date_range(start=max_date + pd.Timedelta(days=1), periods=365)\n",
    "future = pd.DataFrame({'date': future_dates})\n",
    "\n",
    "future['year'] = future['date'].dt.year\n",
    "future['month'] = future['date'].dt.month\n",
    "future['day'] = future['date'].dt.day\n",
    "future['day_of_week'] = future['date'].dt.dayofweek\n",
    "future['quarter'] = future['date'].dt.quarter\n",
    "future['is_weekend'] = future['day_of_week'].isin([5,6]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc19926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use last known lags (simple approximation)\n",
    "last_row = daily_total.iloc[-1]\n",
    "for lag in [1,3,7,14,30]:\n",
    "    future[f'lag_{lag}'] = last_row['item_count']  # placeholder\n",
    "\n",
    "future_pred = best_model.predict(future[features])\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(daily_total['date'], daily_total['item_count'], label='Historical')\n",
    "plt.plot(future['date'], future_pred, label='1-Year Forecast', color='red')\n",
    "plt.title(f'1-Year Sales Forecast using {best_model_name}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90a985f",
   "metadata": {},
   "source": [
    "# 4. Forecasting using Deep Learning (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507c4bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.metrics import mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7efbcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sales amount instead of count\n",
    "daily_amount = df.groupby('date')['sales_amount'].sum().reset_index()\n",
    "daily_amount = daily_amount.set_index('date')\n",
    "\n",
    "# Scale\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(daily_amount)\n",
    "\n",
    "# Create sequences\n",
    "def create_sequences(data, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data)-seq_length):\n",
    "        X.append(data[i:i+seq_length])\n",
    "        y.append(data[i+seq_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "seq_length = 60\n",
    "X, y = create_sequences(scaled, seq_length)\n",
    "\n",
    "# Train-test split (last 12 months as test for synthetic evaluation)\n",
    "total_days = len(daily_amount)\n",
    "test_days = 365\n",
    "train_size = total_days - test_days\n",
    "\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a72a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(seq_length, 1)))\n",
    "model.add(LSTM(50, return_sequences=False))\n",
    "model.add(Dense(25))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=20, verbose=1)\n",
    "\n",
    "# Predict on test\n",
    "pred_scaled = model.predict(X_test)\n",
    "pred = scaler.inverse_transform(pred_scaled)\n",
    "actual = scaler.inverse_transform(y_test)\n",
    "\n",
    "mape = mean_absolute_percentage_error(actual, pred) * 100\n",
    "print(f\"\\nLSTM MAPE on last 12 months (synthetic test): {mape:.2f}%\")\n",
    "print(\"Comment: MAPE < 10% is excellent, < 20% is good, > 30% needs improvement.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f644640a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full training for next 3 months forecast\n",
    "full_model = Sequential()\n",
    "full_model.add(LSTM(50, return_sequences=True, input_shape=(seq_length, 1)))\n",
    "full_model.add(LSTM(50, return_sequences=False))\n",
    "full_model.add(Dense(25))\n",
    "full_model.add(Dense(1))\n",
    "full_model.compile(optimizer='adam', loss='mse')\n",
    "full_model.fit(X, y, batch_size=32, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad7c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast next 90 days\n",
    "last_sequence = scaled[-seq_length:]\n",
    "future_pred = []\n",
    "\n",
    "current_seq = last_sequence.copy()\n",
    "for _ in range(90):\n",
    "    pred = full_model.predict(current_seq.reshape(1, seq_length, 1), verbose=0)\n",
    "    future_pred.append(pred[0,0])\n",
    "    current_seq = np.append(current_seq[1:], pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d9cd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_pred = scaler.inverse_transform(np.array(future_pred).reshape(-1,1))\n",
    "future_dates_90 = pd.date_range(start=daily_amount.index[-1] + pd.Timedelta(days=1), periods=90)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(daily_amount.index[-200:], daily_amount['sales_amount'][-200:], label='Historical')\n",
    "plt.plot(future_dates_90, future_pred, label='Next 3 Months Forecast', color='green')\n",
    "plt.title('LSTM Sales Amount Forecast - Next 3 Months')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
